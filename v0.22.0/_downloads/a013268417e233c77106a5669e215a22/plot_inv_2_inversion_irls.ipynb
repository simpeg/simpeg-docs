{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sparse Inversion with Iteratively Re-Weighted Least-Squares\n\nLeast-squares inversion produces smooth models which may not be an accurate\nrepresentation of the true model. Here we demonstrate the basics of inverting\nfor sparse and/or blocky models. Here, we used the iteratively reweighted\nleast-squares approach. For this tutorial, we focus on the following:\n\n    - Defining the forward problem\n    - Defining the inverse problem (data misfit, regularization, optimization)\n    - Defining the paramters for the IRLS algorithm\n    - Specifying directives for the inversion\n    - Recovering a set of model parameters which explains the observations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom discretize import TensorMesh\n\nfrom simpeg import (\n    simulation,\n    maps,\n    data_misfit,\n    directives,\n    optimization,\n    regularization,\n    inverse_problem,\n    inversion,\n)\n\n# sphinx_gallery_thumbnail_number = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Model and Mapping\n\nHere we generate a synthetic model and a mappig which goes from the model\nspace to the row space of our linear operator.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nParam = 100  # Number of model paramters\n\n# A 1D mesh is used to define the row-space of the linear operator.\nmesh = TensorMesh([nParam])\n\n# Creating the true model\ntrue_model = np.zeros(mesh.nC)\ntrue_model[mesh.cell_centers_x > 0.3] = 1.0\ntrue_model[mesh.cell_centers_x > 0.45] = -0.5\ntrue_model[mesh.cell_centers_x > 0.6] = 0\n\n# Mapping from the model space to the row space of the linear operator\nmodel_map = maps.IdentityMap(mesh)\n\n# Plotting the true model\nfig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(111)\nax.plot(mesh.cell_centers_x, true_model, \"b-\")\nax.set_ylim([-2, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Linear Operator\n\nHere we define the linear operator with dimensions (nData, nParam). In practive,\nyou may have a problem-specific linear operator which you would like to construct\nor load here.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of data observations (rows)\nnData = 20\n\n# Create the linear operator for the tutorial. The columns of the linear operator\n# represents a set of decaying and oscillating functions.\njk = np.linspace(1.0, 60.0, nData)\np = -0.25\nq = 0.25\n\n\ndef g(k):\n    return np.exp(p * jk[k] * mesh.cell_centers_x) * np.cos(\n        np.pi * q * jk[k] * mesh.cell_centers_x\n    )\n\n\nG = np.empty((nData, nParam))\n\nfor i in range(nData):\n    G[i, :] = g(i)\n\n# Plot the columns of G\nfig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(111)\nfor i in range(G.shape[0]):\n    ax.plot(G[i, :])\n\nax.set_title(\"Columns of matrix G\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Simulation\n\nThe simulation defines the relationship between the model parameters and\npredicted data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim = simulation.LinearSimulation(mesh, G=G, model_map=model_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Synthetic Data\n\nHere, we use the true model to create synthetic data which we will subsequently\ninvert.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Standard deviation of Gaussian noise being added\nstd = 0.02\nnp.random.seed(1)\n\n# Create a SimPEG data object\ndata_obj = sim.make_synthetic_data(true_model, noise_floor=std, add_noise=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Inverse Problem\n\nThe inverse problem is defined by 3 things:\n\n    1) Data Misfit: a measure of how well our recovered model explains the field data\n    2) Regularization: constraints placed on the recovered model and a priori information\n    3) Optimization: the numerical approach used to solve the inverse problem\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the data misfit. Here the data misfit is the L2 norm of the weighted\n# residual between the observed data and the data predicted for a given model.\n# Within the data misfit, the residual between predicted and observed data are\n# normalized by the data's standard deviation.\ndmis = data_misfit.L2DataMisfit(simulation=sim, data=data_obj)\n\n# Define the regularization (model objective function). Here, 'p' defines the\n# the norm of the smallness term and 'q' defines the norm of the smoothness\n# term.\nreg = regularization.Sparse(mesh, mapping=model_map)\nreg.reference_model = np.zeros(nParam)\np = 0.0\nq = 0.0\nreg.norms = [p, q]\n\n# Define how the optimization problem is solved.\nopt = optimization.ProjectedGNCG(\n    maxIter=100, lower=-2.0, upper=2.0, maxIterLS=20, maxIterCG=30, tolCG=1e-4\n)\n\n# Here we define the inverse problem that is to be solved\ninv_prob = inverse_problem.BaseInvProblem(dmis, reg, opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Inversion Directives\n\nHere we define any directiveas that are carried out during the inversion. This\nincludes the cooling schedule for the trade-off parameter (beta), stopping\ncriteria for the inversion and saving inversion results at each iteration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Add sensitivity weights but don't update at each beta\nsensitivity_weights = directives.UpdateSensitivityWeights(every_iteration=False)\n\n# Reach target misfit for L2 solution, then use IRLS until model stops changing.\nIRLS = directives.Update_IRLS(max_irls_iterations=40, minGNiter=1, f_min_change=1e-4)\n\n# Defining a starting value for the trade-off parameter (beta) between the data\n# misfit and the regularization.\nstarting_beta = directives.BetaEstimate_ByEig(beta0_ratio=1e0)\n\n# Update the preconditionner\nupdate_Jacobi = directives.UpdatePreconditioner()\n\n# Save output at each iteration\nsaveDict = directives.SaveOutputEveryIteration(save_txt=False)\n\n# Define the directives as a list\ndirectives_list = [sensitivity_weights, IRLS, starting_beta, update_Jacobi, saveDict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting a Starting Model and Running the Inversion\n\nTo define the inversion object, we need to define the inversion problem and\nthe set of directives. We can then run the inversion.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we combine the inverse problem and the set of directives\ninv = inversion.BaseInversion(inv_prob, directives_list)\n\n# Starting model\nstarting_model = 1e-4 * np.ones(nParam)\n\n# Run inversion\nrecovered_model = inv.run(starting_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12 * 1.2, 4 * 1.2))\n\n# True versus recovered model\nax[0].plot(mesh.cell_centers_x, true_model, \"k-\")\nax[0].plot(mesh.cell_centers_x, inv_prob.l2model, \"b-\")\nax[0].plot(mesh.cell_centers_x, recovered_model, \"r-\")\nax[0].legend((\"True Model\", \"Recovered L2 Model\", \"Recovered Sparse Model\"))\nax[0].set_ylim([-2, 2])\n\n# Observed versus predicted data\nax[1].plot(data_obj.dobs, \"k-\")\nax[1].plot(inv_prob.dpred, \"ko\")\nax[1].legend((\"Observed Data\", \"Predicted Data\"))\n\n# Plot convergence\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_axes([0.2, 0.1, 0.7, 0.85])\nax.plot(saveDict.phi_d, \"k\", lw=2)\n\ntwin = ax.twinx()\ntwin.plot(saveDict.phi_m, \"k--\", lw=2)\nax.plot(np.r_[IRLS.iterStart, IRLS.iterStart], np.r_[0, np.max(saveDict.phi_d)], \"k:\")\nax.text(\n    IRLS.iterStart,\n    0.0,\n    \"IRLS Start\",\n    va=\"bottom\",\n    ha=\"center\",\n    rotation=\"vertical\",\n    size=12,\n    bbox={\"facecolor\": \"white\"},\n)\n\nax.set_ylabel(r\"$\\phi_d$\", size=16, rotation=0)\nax.set_xlabel(\"Iterations\", size=14)\ntwin.set_ylabel(r\"$\\phi_m$\", size=16, rotation=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}